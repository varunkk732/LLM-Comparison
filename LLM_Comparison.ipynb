{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNTBousW2RZo2dZL2UQj9Fa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunkk732/LLM-Comparison/blob/main/LLM_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"llm_comparison_dataset.csv\")\n",
        "\n",
        "# Define aggregation rules\n",
        "agg_rules = {\n",
        "    \"Provider\": lambda x: x.mode()[0],  # Most frequent provider\n",
        "    \"Context Window\": \"mean\",\n",
        "    \"Speed (tokens/sec)\": \"mean\",\n",
        "    \"Latency (sec)\": \"mean\",\n",
        "    \"Benchmark (MMLU)\": \"mean\",\n",
        "    \"Benchmark (Chatbot Arena)\": \"mean\",\n",
        "    \"Open-Source\": lambda x: x.mode()[0],  # Most frequent value\n",
        "    \"Price / Million Tokens\": \"mean\",\n",
        "    \"Training Dataset Size\": \"mean\",\n",
        "    \"Compute Power\": \"mean\",\n",
        "    \"Energy Efficiency\": \"mean\",\n",
        "    \"Quality Rating\": \"mean\",\n",
        "    \"Speed Rating\": \"mean\",\n",
        "    \"Price Rating\": \"mean\"\n",
        "}\n",
        "\n",
        "# Aggregate data by model name\n",
        "df_agg = df.groupby(\"Model\").agg(agg_rules).reset_index()\n",
        "\n",
        "# Display the new dataset structure\n",
        "df_agg.head()\n",
        "\n",
        "# Assuming you have the aggregated DataFrame `df_agg`\n",
        "df_agg.to_csv(\"llm_comparison_aggregated.csv\", index=False)\n",
        "\n",
        "print(\"File saved as llm_comparison_aggregated.csv\")"
      ],
      "metadata": {
        "id": "cUi7e7XPb4BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your aggregated dataset\n",
        "df_agg = pd.read_csv(\"llm_comparison_aggregated.csv\")\n",
        "\n",
        "# Select numerical columns (excluding categorical ones)\n",
        "num_cols = df_agg.select_dtypes(include=[\"int64\", \"float64\"]).columns.difference([\"Open-Source\"])\n",
        "\n",
        "# Apply Min-Max Scaling (0-1)\n",
        "scaler = MinMaxScaler()\n",
        "df_agg[num_cols] = scaler.fit_transform(df_agg[num_cols])\n",
        "\n",
        "# Save the scaled dataset\n",
        "df_agg.to_csv(\"llm_comparison_scaled.csv\", index=False)\n",
        "\n",
        "print(\"Min-Max Scaling applied and saved as llm_comparison_scaled.csv\")\n"
      ],
      "metadata": {
        "id": "_TXiT66Xd8p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('llm_comparison_aggregated.csv')\n",
        "\n",
        "# Display the first few rows before standardization\n",
        "print(\"Original Data (First 5 rows):\")\n",
        "print(data.head())\n",
        "\n",
        "# Columns to standardize (exclude categorical columns)\n",
        "numeric_columns = [\n",
        "    'Context Window', 'Speed (tokens/sec)', 'Latency (sec)',\n",
        "    'Benchmark (MMLU)', 'Benchmark (Chatbot Arena)',\n",
        "    'Price / Million Tokens', 'Training Dataset Size',\n",
        "    'Compute Power', 'Energy Efficiency',\n",
        "    'Quality Rating', 'Speed Rating', 'Price Rating'\n",
        "]\n",
        "\n",
        "# Create a copy for the standardized data\n",
        "data_standardized = data.copy()\n",
        "\n",
        "# Apply z-score standardization to numeric columns\n",
        "for column in numeric_columns:\n",
        "    mean = data[column].mean()\n",
        "    std = data[column].std()\n",
        "    data_standardized[column] = (data[column] - mean) / std\n",
        "\n",
        "# Display the first few rows after standardization\n",
        "print(\"\\nStandardized Data (First 5 rows):\")\n",
        "print(data_standardized.head())\n",
        "\n",
        "# Summary statistics for standardized columns\n",
        "print(\"\\nSummary Statistics for Standardized Columns:\")\n",
        "print(data_standardized[numeric_columns].describe().T[['mean', 'std']])\n",
        "\n",
        "# Save the standardized data to a new CSV file\n",
        "data_standardized.to_csv('llm_comparison_standardized.csv', index=False)\n",
        "print(\"\\nStandardized data saved to 'llm_comparison_standardized.csv'\")"
      ],
      "metadata": {
        "id": "S79KqOSCfTjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data to encode\n",
        "data = pd.read_csv('llm_comparison_standardized.csv')\n",
        "\n",
        "# Create a copy to work with\n",
        "encoded_data = data.copy()\n",
        "\n",
        "# MORE EFFICIENT ENCODING for Model using Provider information\n",
        "print(\"\\nEFFICIENT ENCODING FOR MODEL\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Extract model number from the model name and combine with provider\n",
        "# Example: convert \"Claude-1\", \"GPT-2\", etc. to just 1, 2, etc.\n",
        "encoded_data['Model_number'] = encoded_data['Model'].str.extract(r'-(\\d+)').astype(int)\n",
        "\n",
        "# Create a more compact mapping by combining provider and model number\n",
        "# This means model 1 from different providers will have different codes\n",
        "unique_providers = data['Provider'].unique()\n",
        "provider_code_map = {provider: i for i, provider in enumerate(unique_providers)}\n",
        "\n",
        "# Map providers to their numeric codes\n",
        "encoded_data['Provider_code'] = encoded_data['Provider'].map(provider_code_map)\n",
        "\n",
        "# Print provider mapping\n",
        "print(\"Provider Code Mapping:\")\n",
        "for provider, code in provider_code_map.items():\n",
        "    print(f\"  {provider} -> {code}\")\n",
        "\n",
        "# Create a combined model+provider code\n",
        "# Formula: provider_code * 100 + model_number\n",
        "# This gives each unique model a unique code while keeping related models close\n",
        "encoded_data['Model_encoded'] = encoded_data['Provider_code'] * 100 + encoded_data['Model_number']\n",
        "\n",
        "# Create and print the mapping\n",
        "model_mapping = {}\n",
        "for _, row in encoded_data[['Model', 'Model_encoded']].drop_duplicates().iterrows():\n",
        "    model_mapping[row['Model']] = row['Model_encoded']\n",
        "\n",
        "print(\"\\nEfficient Model Encoding Mapping:\")\n",
        "for original, encoded in model_mapping.items():\n",
        "    print(f\"  {original} -> {encoded}\")\n",
        "\n",
        "# Show a subset of the encoded data\n",
        "print(\"\\nFirst 5 rows of encoded data:\")\n",
        "cols_to_show = ['Model', 'Model_encoded', 'Provider', 'Provider_code', 'Model_number']\n",
        "print(encoded_data[cols_to_show].head())\n",
        "\n",
        "# Save the encoded dataset\n",
        "encoded_data.to_csv('llm_comparison_efficient_encoded.csv', index=False)\n",
        "print(\"\\nEncoded data saved to 'llm_comparison_efficient_encoded.csv'\")"
      ],
      "metadata": {
        "id": "9A06h8hmgWuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Performance Prediction Project\n",
        "# This script builds models to predict LLM benchmark performance based on various features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the data\n",
        "df_scaled = pd.read_csv('llm_comparison_scaled_encoded.csv')\n",
        "df_standardized = pd.read_csv('llm_comparison_standardized_encoded.csv')\n",
        "\n",
        "# Let's work with the standardized data\n",
        "df = df_standardized.copy()\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nSample Data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Extract provider names for later use\n",
        "providers = {code: name for code, name in zip(df['Provider_code'].unique(), df['Provider'].unique())}\n",
        "\n",
        "# Data Exploration\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# Distribution of benchmark scores\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Benchmark (MMLU)'], kde=True)\n",
        "plt.title('Distribution of MMLU Benchmark Scores')\n",
        "plt.xlabel('Standardized Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('benchmark_distributions.png')\n",
        "plt.close()\n",
        "\n",
        "# Correlation matrix for key features\n",
        "feature_cols = ['Context Window', 'Speed (tokens/sec)', 'Latency (sec)',\n",
        "                'Price / Million Tokens', 'Training Dataset Size',\n",
        "                'Compute Power', 'Energy Efficiency', 'Open-Source']\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation = df[feature_cols + ['Benchmark (MMLU)']].corr()\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# Box plots for benchmark scores by provider\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "sns.boxplot(x='Provider', y='Benchmark (MMLU)', data=df)\n",
        "plt.title('MMLU Benchmark Scores by Provider')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('benchmark_by_provider.png')\n",
        "plt.close()\n",
        "\n",
        "# Scatter plots to explore relationships between key features and benchmarks\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "for i, feature in enumerate(feature_cols[:4]):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    sns.scatterplot(x=feature, y='Benchmark (MMLU)', data=df, hue='Provider')\n",
        "    plt.title(f'{feature} vs MMLU Benchmark')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_vs_mmlu.png')\n",
        "plt.close()\n",
        "\n",
        "# Feature Engineering\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# Create combined feature for efficiency (speed/energy consumption)\n",
        "df['Efficiency_Ratio'] = df['Speed (tokens/sec)'] / (df['Energy Efficiency'] + 3)  # Adding 3 to avoid division by negative values\n",
        "\n",
        "# Create pricing tier categories\n",
        "df['Price_Tier'] = pd.qcut(df['Price / Million Tokens'], q=3, labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "# Create model generation indicator\n",
        "df['Generation'] = df['Model_number']\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "df_encoded = pd.get_dummies(df, columns=['Provider', 'Price_Tier'], drop_first=True)\n",
        "\n",
        "# Prepare features and targets\n",
        "X = df_encoded.drop(['Model', 'Benchmark (MMLU)', 'Model_encoded', 'Provider_code'], axis=1)\n",
        "y_mmlu = df_encoded['Benchmark (MMLU)']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_mmlu_train, y_mmlu_test = train_test_split(X, y_mmlu, test_size=0.25, random_state=42)\n",
        "\n",
        "# Model Building - For MMLU Benchmark\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# Define models to evaluate\n",
        "models_mmlu = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(),\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Function to evaluate models\n",
        "def evaluate_models(models, X_train, X_test, y_train, y_test):\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'mse': mse,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'r2': r2\n",
        "        }\n",
        "\n",
        "        print(f\"{name} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Evaluate models for MMLU\n",
        "print(\"\\nEvaluating models for MMLU benchmark prediction:\")\n",
        "mmlu_results = evaluate_models(models_mmlu, X_train, X_test, y_mmlu_train, y_mmlu_test)\n",
        "\n",
        "# Find best models\n",
        "best_mmlu_model = max(mmlu_results.items(), key=lambda x: x[1]['r2'])\n",
        "\n",
        "print(f\"\\nBest model for MMLU prediction: {best_mmlu_model[0]} with R² = {best_mmlu_model[1]['r2']:.4f}\")\n",
        "\n",
        "# Feature Importance Analysis\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# For the best models, extract feature importance if available\n",
        "def plot_feature_importance(model, model_name, X_columns, target_name):\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importance = model.feature_importances_\n",
        "\n",
        "        # Create DataFrame for feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': X_columns,\n",
        "            'Importance': importance\n",
        "        })\n",
        "\n",
        "        # Sort by importance\n",
        "        feature_importance = feature_importance.sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "        plt.title(f'Feature Importance for {target_name} using {model_name}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'feature_importance_{target_name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "        return feature_importance\n",
        "    else:\n",
        "        print(f\"Model {model_name} doesn't have feature_importances_ attribute\")\n",
        "        return None\n",
        "\n",
        "# Extract best models\n",
        "best_mmlu_model_name = best_mmlu_model[0]\n",
        "best_mmlu_model_obj = best_mmlu_model[1]['model']\n",
        "\n",
        "# Plot feature importance\n",
        "mmlu_importance = plot_feature_importance(best_mmlu_model_obj, best_mmlu_model_name, X.columns, 'MMLU Benchmark')\n",
        "\n",
        "if mmlu_importance is not None:\n",
        "    print(\"\\nTop 5 features for MMLU prediction:\")\n",
        "    print(mmlu_importance.head())\n",
        "\n",
        "# Model Tuning\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# Function to perform grid search for hyperparameter tuning\n",
        "def tune_model(model, param_grid, X_train, y_train):\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation R²: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# Define parameter grids for the best models\n",
        "if best_mmlu_model_name == 'Random Forest':\n",
        "    param_grid_mmlu = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    print(\"\\nTuning Random Forest for MMLU prediction:\")\n",
        "    tuned_mmlu_model = tune_model(RandomForestRegressor(random_state=42), param_grid_mmlu, X_train, y_mmlu_train)\n",
        "elif best_mmlu_model_name == 'Gradient Boosting':\n",
        "    param_grid_mmlu = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "    print(\"\\nTuning Gradient Boosting for MMLU prediction:\")\n",
        "    tuned_mmlu_model = tune_model(GradientBoostingRegressor(random_state=42), param_grid_mmlu, X_train, y_mmlu_train)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "y_mmlu_pred_tuned = tuned_mmlu_model.predict(X_test)\n",
        "tuned_mse = mean_squared_error(y_mmlu_test, y_mmlu_pred_tuned)\n",
        "tuned_rmse = np.sqrt(tuned_mse)\n",
        "tuned_r2 = r2_score(y_mmlu_test, y_mmlu_pred_tuned)\n",
        "\n",
        "print(f\"\\nTuned model performance for MMLU prediction:\")\n",
        "print(f\"RMSE: {tuned_rmse:.4f}, R²: {tuned_r2:.4f}\")\n",
        "\n",
        "# Predicted vs Actual Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_mmlu_test, y_mmlu_pred_tuned, alpha=0.7)\n",
        "plt.plot([y_mmlu_test.min(), y_mmlu_test.max()], [y_mmlu_test.min(), y_mmlu_test.max()], 'r--')\n",
        "plt.xlabel('Actual MMLU Benchmark Score')\n",
        "plt.ylabel('Predicted MMLU Benchmark Score')\n",
        "plt.title('Predicted vs Actual MMLU Benchmark Scores (Tuned Model)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('predicted_vs_actual_mmlu.png')\n",
        "plt.close()\n",
        "\n",
        "# Model for future LLM prediction\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# Train the final model on the full dataset\n",
        "final_model = tuned_mmlu_model\n",
        "final_model.fit(X, y_mmlu)\n",
        "\n",
        "# Save insights and findings\n",
        "insights = \"\"\"\n",
        "# LLM Performance Prediction - Key Insights\n",
        "\n",
        "## Model Performance\n",
        "- The best model for predicting MMLU benchmark scores is {best_model} with an R² of {r2:.4f}.\n",
        "- After tuning, the model achieved an R² of {tuned_r2:.4f}.\n",
        "\n",
        "## Key Factors Influencing LLM Performance\n",
        "Based on feature importance analysis, the top factors that influence MMLU benchmark scores are:\n",
        "{top_features}\n",
        "\n",
        "## Provider Comparison\n",
        "- The providers with the highest average MMLU scores are: {top_providers_mmlu}\n",
        "- The providers with the highest average Chatbot Arena scores are: {top_providers_arena}\n",
        "\n",
        "## Interesting Findings\n",
        "- There is a {correlation_compute_mmlu:.2f} correlation between compute power and MMLU scores.\n",
        "- Open-source models show {open_source_diff:.2f} difference in average performance compared to closed-source models.\n",
        "- Models with larger context windows tend to {context_window_impact} in benchmark scores.\n",
        "\n",
        "## Recommendations\n",
        "1. Focus on {recommendation1} to maximize benchmark performance.\n",
        "2. The trade-off between {recommendation2} should be carefully considered when developing new models.\n",
        "3. For cost-efficient performance, prioritize {recommendation3}.\n",
        "\"\"\"\n",
        "\n",
        "# Fill in the insights template with actual values\n",
        "provider_mmlu_avg = df.groupby('Provider')['Benchmark (MMLU)'].mean().sort_values(ascending=False)\n",
        "provider_arena_avg = df.groupby('Provider')['Benchmark (Chatbot Arena)'].mean().sort_values(ascending=False)\n",
        "\n",
        "top_providers_mmlu = \", \".join(provider_mmlu_avg.index[:3])\n",
        "top_providers_arena = \", \".join(provider_arena_avg.index[:3])\n",
        "\n",
        "correlation_compute_mmlu = correlation.loc['Compute Power', 'Benchmark (MMLU)']\n",
        "open_source_diff = df[df['Open-Source'] == 1]['Benchmark (MMLU)'].mean() - df[df['Open-Source'] == 0]['Benchmark (MMLU)'].mean()\n",
        "\n",
        "if correlation.loc['Context Window', 'Benchmark (MMLU)'] > 0:\n",
        "    context_window_impact = \"show improvement\"\n",
        "else:\n",
        "    context_window_impact = \"do not necessarily show improvement\"\n",
        "\n",
        "if mmlu_importance is not None:\n",
        "    top_features = \"\\n\".join([f\"- {row['Feature']}: {row['Importance']:.4f}\" for _, row in mmlu_importance.head(5).iterrows()])\n",
        "else:\n",
        "    top_features = \"Feature importance analysis not available for the selected model.\"\n",
        "\n",
        "# Determine recommendations based on analysis\n",
        "if mmlu_importance is not None and 'Training Dataset Size' in mmlu_importance['Feature'].values[:5]:\n",
        "    recommendation1 = \"increasing training dataset size\"\n",
        "elif mmlu_importance is not None and 'Compute Power' in mmlu_importance['Feature'].values[:5]:\n",
        "    recommendation1 = \"allocating more compute resources\"\n",
        "else:\n",
        "    recommendation1 = \"optimizing model architecture\"\n",
        "\n",
        "recommendation2 = \"inference speed and model accuracy\"\n",
        "recommendation3 = \"models with balanced compute efficiency and training dataset size\"\n",
        "\n",
        "filled_insights = insights.format(\n",
        "    best_model=best_mmlu_model_name,\n",
        "    r2=best_mmlu_model[1]['r2'],\n",
        "    tuned_r2=tuned_r2,\n",
        "    top_features=top_features,\n",
        "    top_providers_mmlu=top_providers_mmlu,\n",
        "    top_providers_arena=top_providers_arena,\n",
        "    correlation_compute_mmlu=correlation_compute_mmlu,\n",
        "    open_source_diff=open_source_diff,\n",
        "    context_window_impact=context_window_impact,\n",
        "    recommendation1=recommendation1,\n",
        "    recommendation2=recommendation2,\n",
        "    recommendation3=recommendation3\n",
        ")\n",
        "\n",
        "with open('llm_performance_insights.md', 'w') as f:\n",
        "    f.write(filled_insights)\n",
        "\n",
        "print(\"\\nProject completed successfully! Results and insights saved to files.\")"
      ],
      "metadata": {
        "id": "AyzHe-Yj-Sjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Load the standardized encoded data\n",
        "df_encoded = pd.read_csv('llm_comparison_standardized_encoded.csv')\n",
        "\n",
        "# Load the scaled data\n",
        "df_scaled = pd.read_csv('llm_comparison_scaled_encoded.csv')\n",
        "\n",
        "# Create a model identifier column\n",
        "df_encoded['Model_ID'] = df_encoded['Model'] + ' (' + df_encoded['Provider'] + ')'\n",
        "df_scaled['Model_ID'] = df_scaled['Model'] + ' (' + df_scaled['Provider'] + ')'\n",
        "\n",
        "# Display basic info about the datasets\n",
        "print(f\"Encoded dataset shape: {df_encoded.shape}\")\n",
        "print(f\"Scaled dataset shape: {df_scaled.shape}\")\n",
        "\n",
        "# Summary statistics of encoded data\n",
        "encoded_summary = df_encoded.describe()\n",
        "\n",
        "# Visualize distributions of key metrics\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "features = ['Context Window', 'Speed (tokens/sec)', 'Latency (sec)',\n",
        "            'Benchmark (MMLU)', 'Benchmark (Chatbot Arena)',\n",
        "            'Price / Million Tokens', 'Training Dataset Size',\n",
        "            'Compute Power', 'Energy Efficiency']\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    sns.histplot(df_encoded[feature], ax=axes[i], kde=True)\n",
        "    axes[i].set_title(f'Distribution of {feature}')\n",
        "    axes[i].set_xlabel(feature)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_distributions.png')\n",
        "\n",
        "# Correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = df_encoded[features].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix of LLM Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png')\n",
        "\n",
        "# Compare models by provider\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(x='Provider', y='Benchmark (MMLU)', data=df_encoded)\n",
        "plt.title('MMLU Benchmark Scores by Provider')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('mmlu_by_provider.png')\n",
        "\n",
        "# Quality vs Speed by Provider\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.scatterplot(data=df_encoded, x='Quality Rating', y='Speed Rating',\n",
        "                hue='Provider', size='Benchmark (MMLU)', sizes=(50, 200))\n",
        "plt.title('Quality vs Speed Rating by Provider')\n",
        "plt.tight_layout()\n",
        "plt.savefig('quality_vs_speed.png')\n",
        "\n",
        "# Select features for clustering\n",
        "clustering_features = [\n",
        "    'Context Window', 'Speed (tokens/sec)', 'Latency (sec)',\n",
        "    'Benchmark (MMLU)', 'Benchmark (Chatbot Arena)',\n",
        "    'Price / Million Tokens', 'Training Dataset Size',\n",
        "    'Compute Power', 'Energy Efficiency',\n",
        "    'Quality Rating', 'Speed Rating', 'Price Rating'\n",
        "]\n",
        "\n",
        "# Extract features for clustering (already standardized in the encoded dataset)\n",
        "X = df_encoded[clustering_features].values\n",
        "\n",
        "# Handle missing values if any\n",
        "X = np.nan_to_num(X)\n",
        "\n",
        "# Elbow method to determine optimal number of clusters\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "k_range = range(2, 10)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    if k > 1:  # Silhouette score requires at least 2 clusters\n",
        "        labels = kmeans.labels_\n",
        "        silhouette_scores.append(silhouette_score(X, labels))\n",
        "\n",
        "# Plot elbow curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia (Sum of squared distances)')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.grid(True)\n",
        "plt.savefig('elbow_method.png')\n",
        "\n",
        "# Hierarchical clustering dendrogram\n",
        "plt.figure(figsize=(16, 10))\n",
        "linked = linkage(X, method='ward')\n",
        "dendrogram(linked, labels=df_encoded['Model_ID'].values, orientation='top',\n",
        "           leaf_font_size=8, distance_sort='descending')\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Distance')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('hierarchical_dendrogram.png')\n",
        "\n",
        "# Apply K-means with optimal K (assuming k=4 based on analysis)\n",
        "optimal_k = 4\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Add cluster labels to dataframe\n",
        "df_encoded['KMeans_Cluster'] = kmeans_labels\n",
        "df_scaled['KMeans_Cluster'] = kmeans_labels\n",
        "\n",
        "# Apply hierarchical clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
        "hierarchical_labels = hierarchical.fit_predict(X)\n",
        "\n",
        "# Add cluster labels to dataframe\n",
        "df_encoded['Hierarchical_Cluster'] = hierarchical_labels\n",
        "df_scaled['Hierarchical_Cluster'] = hierarchical_labels\n",
        "\n",
        "# Apply PCA for visualization\n",
        "pca = PCA(n_components=3)\n",
        "pca_result = pca.fit_transform(X)\n",
        "\n",
        "# Create a DataFrame with PCA results\n",
        "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2', 'PC3'])\n",
        "pca_df['KMeans_Cluster'] = kmeans_labels\n",
        "pca_df['Hierarchical_Cluster'] = hierarchical_labels\n",
        "pca_df['Model'] = df_encoded['Model_ID']\n",
        "pca_df['Provider'] = df_encoded['Provider']\n",
        "\n",
        "# PCA explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f\"PCA explained variance ratio: {explained_variance}\")\n",
        "print(f\"Total variance explained: {sum(explained_variance):.2f}\")\n",
        "\n",
        "# Visualize PCA components with K-means clusters\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.scatterplot(x='PC1', y='PC2', hue='KMeans_Cluster',\n",
        "                palette='viridis', s=100, data=pca_df)\n",
        "\n",
        "# Add model names as annotations\n",
        "for idx, row in pca_df.iterrows():\n",
        "    plt.annotate(row['Model'], (row['PC1'], row['PC2']),\n",
        "                 fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.title('PCA of LLMs with K-means Clusters')\n",
        "plt.tight_layout()\n",
        "plt.savefig('pca_kmeans_clusters.png')\n",
        "\n",
        "# 3D PCA plot with plotly\n",
        "fig = px.scatter_3d(pca_df, x='PC1', y='PC2', z='PC3',\n",
        "                    color='KMeans_Cluster', symbol='Provider',\n",
        "                    hover_name='Model', title='3D PCA with K-means Clusters')\n",
        "fig.update_layout(scene=dict(xaxis_title='PC1', yaxis_title='PC2', zaxis_title='PC3'))\n",
        "fig.write_html('3d_pca_clusters.html')\n",
        "\n",
        "# Analyze feature importance in PCA\n",
        "pca_components = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    columns=[f'PC{i+1}' for i in range(3)],\n",
        "    index=clustering_features\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(pca_components, annot=True, cmap='coolwarm')\n",
        "plt.title('Feature Importance in Principal Components')\n",
        "plt.tight_layout()\n",
        "plt.savefig('pca_feature_importance.png')\n",
        "\n",
        "# Calculate cluster profiles\n",
        "cluster_profiles = df_encoded.groupby('KMeans_Cluster')[clustering_features].mean()\n",
        "\n",
        "# Radar chart for cluster profiles\n",
        "categories = clustering_features\n",
        "fig = make_subplots(rows=2, cols=2, specs=[[{'type': 'polar'}]*2]*2,\n",
        "                   subplot_titles=[f'Cluster {i}' for i in range(optimal_k)])\n",
        "\n",
        "for i in range(optimal_k):\n",
        "    row, col = divmod(i, 2)\n",
        "    values = cluster_profiles.iloc[i].values.tolist()\n",
        "    values.append(values[0])  # Close the loop\n",
        "\n",
        "    cats = categories + [categories[0]]  # Close the loop\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatterpolar(\n",
        "            r=values,\n",
        "            theta=cats,\n",
        "            fill='toself',\n",
        "            name=f'Cluster {i}'\n",
        "        ),\n",
        "        row=row+1, col=col+1\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800,\n",
        "    width=1000,\n",
        "    title_text='Cluster Profiles'\n",
        ")\n",
        "fig.write_html('cluster_profiles_radar.html')\n",
        "\n",
        "# Bar chart comparing key features across clusters\n",
        "plt.figure(figsize=(15, 10))\n",
        "cluster_profiles[['Quality Rating', 'Speed Rating', 'Price Rating',\n",
        "                  'Benchmark (MMLU)', 'Benchmark (Chatbot Arena)']].plot(\n",
        "    kind='bar', figsize=(15, 8))\n",
        "plt.title('Key Performance Metrics by Cluster')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Average Value (Standardized)')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cluster_key_metrics.png')\n",
        "\n",
        "# Count models per provider in each cluster\n",
        "provider_cluster_counts = pd.crosstab(df_encoded['Provider'], df_encoded['KMeans_Cluster'])\n",
        "provider_cluster_counts.plot(kind='bar', stacked=True, figsize=(12, 8))\n",
        "plt.title('Distribution of Providers Across Clusters')\n",
        "plt.xlabel('Provider')\n",
        "plt.ylabel('Number of Models')\n",
        "plt.tight_layout()\n",
        "plt.savefig('provider_cluster_distribution.png')\n",
        "\n",
        "# Display detailed statistics for each cluster\n",
        "for cluster in range(optimal_k):\n",
        "    print(f\"\\n=== Cluster {cluster} ===\")\n",
        "    cluster_models = df_encoded[df_encoded['KMeans_Cluster'] == cluster]['Model_ID'].values\n",
        "    print(f\"Models in this cluster: {cluster_models}\")\n",
        "\n",
        "    print(\"\\nCluster Statistics:\")\n",
        "    cluster_stats = df_encoded[df_encoded['KMeans_Cluster'] == cluster][clustering_features].describe()\n",
        "    print(cluster_stats)\n",
        "\n",
        "# Identify top models in each cluster based on different criteria\n",
        "best_mmlu = df_encoded.loc[df_encoded.groupby('KMeans_Cluster')['Benchmark (MMLU)'].idxmax()]\n",
        "best_arena = df_encoded.loc[df_encoded.groupby('KMeans_Cluster')['Benchmark (Chatbot Arena)'].idxmax()]\n",
        "best_quality = df_encoded.loc[df_encoded.groupby('KMeans_Cluster')['Quality Rating'].idxmax()]\n",
        "best_efficiency = df_encoded.loc[df_encoded.groupby('KMeans_Cluster')['Energy Efficiency'].idxmax()]\n",
        "\n",
        "print(\"\\n=== Best-in-class Models by Cluster ===\")\n",
        "print(\"\\nBest MMLU Score:\")\n",
        "for _, row in best_mmlu.iterrows():\n",
        "    print(f\"Cluster {row['KMeans_Cluster']}: {row['Model_ID']} (Score: {row['Benchmark (MMLU)']})\")\n",
        "\n",
        "print(\"\\nBest Chatbot Arena Score:\")\n",
        "for _, row in best_arena.iterrows():\n",
        "    print(f\"Cluster {row['KMeans_Cluster']}: {row['Model_ID']} (Score: {row['Benchmark (Chatbot Arena)']})\")\n",
        "\n",
        "print(\"\\nBest Quality Rating:\")\n",
        "for _, row in best_quality.iterrows():\n",
        "    print(f\"Cluster {row['KMeans_Cluster']}: {row['Model_ID']} (Rating: {row['Quality Rating']})\")\n",
        "\n",
        "print(\"\\nBest Energy Efficiency:\")\n",
        "for _, row in best_efficiency.iterrows():\n",
        "    print(f\"Cluster {row['KMeans_Cluster']}: {row['Model_ID']} (Efficiency: {row['Energy Efficiency']})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jpzVr8MACJvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}